{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = '/kaggle/working/Combined/Train'\nactivities = sorted([act for act in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, act))])\n\nactivity_counts = {}\n\nfor activity in activities:\n    activity_counts[activity] = len([f for f in os.listdir(os.path.join(base_path, activity)) if f.endswith('.csv')])\n\n# Plot\nplt.figure(figsize=(8, 4))\nsns.barplot(x=list(activity_counts.keys()), y=list(activity_counts.values()))\nplt.xticks(rotation=45)\nplt.title(\"Number of Samples per Activity (Train Set)\")\nplt.ylabel(\"Sample Count\")\nplt.xlabel(\"Activity\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_file = os.listdir(os.path.join(base_path, 'WALKING'))[0]\ndf_sample = pd.read_csv(os.path.join(base_path, 'WALKING', sample_file), header=None)\n\nprint(f\"Sample shape: {df_sample.shape}\")\ndf_sample.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from glob import glob\n\nplt.figure(figsize=(15, 8))\n\nfor i, activity in enumerate(activities):\n    sample_path = glob(os.path.join(base_path, activity, '*.csv'))[0]\n    df = pd.read_csv(sample_path, header=None)\n\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n\n    total_acc = np.sqrt((df**2).sum(axis=1))\n\n    plt.subplot(2, 3, i+1)\n    plt.plot(total_acc)\n    plt.title(activity)\n    plt.xticks([])\n\nplt.suptitle(\"Total Acceleration per Sample by Activity\", y=1.02)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.title(\"Correlation between acceleration axes\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for activity in activities:\n    path = os.path.join(base_path, activity)\n    file = os.listdir(path)[0]\n    df = pd.read_csv(os.path.join(path, file), header=None)\n\n    # Convert to numeric safely\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n\n    df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n    print(f\"{activity:<20} | Mean Acc Magnitude: {df['acc_mag'].mean():.2f} | Std: {df['acc_mag'].std():.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\n\nfig = plt.figure(figsize=(18, 10))\n\nfor i, activity in enumerate(activities):\n    path = os.path.join(base_path, activity)\n    file = os.listdir(path)[5]\n    df = pd.read_csv(os.path.join(path, file), header=None)\n\n    # Convert to numeric and clean\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n    df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n\n    # Subplot\n    ax = fig.add_subplot(2, 3, i+1, projection='3d')\n    ax.plot(df['acc_x'], df['acc_y'], df['acc_z'], linewidth=0.8)\n    ax.set_title(f\"{activity}\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_zlabel(\"Z\")\n\nplt.suptitle(\"3D Trajectories of Acceleration for Each Activity\", y=1.02)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for activity in activities:\n    path = os.path.join(base_path, activity)\n    file = os.listdir(path)[0]\n    df = pd.read_csv(os.path.join(path, file), header=None)\n\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n    df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n\n    plt.figure(figsize=(10, 4))\n    plt.plot(df['acc_x'], label='X')\n    plt.plot(df['acc_y'], label='Y')\n    plt.plot(df['acc_z'], label='Z')\n    plt.title(f\"Waveform for {activity}\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Acceleration\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Across all activities (LAYING, SITTING, STANDING, WALKING, etc.), the mean acceleration magnitude is observed to be close to 1.0, with very low standard deviation for static activities and slightly higher values for dynamic ones.\n\nA device at rest under Earth's gravity experiences a net acceleration of ~1g.\n\nThus, during stationary activities like LAYING, SITTING, the only significant acceleration is due to gravity, resulting in a stable magnitude of ~1.0g. During dynamic activities like WALKING, additional body motion introduces variability, increasing the standard deviation.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX = []\ny = []\n\nfor activity in activities:\n    path = os.path.join(base_path, activity)\n    for file in os.listdir(path)[:21]:  # pick all 21 samples\n        df = pd.read_csv(os.path.join(path, file), header=None)\n        df = df.apply(pd.to_numeric, errors='coerce')\n        df.dropna(inplace=True)\n        df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n        \n        # Use mean acceleration vector of the sample\n        sample_feature = df.mean().values  # shape (3,)\n        X.append(sample_feature)\n        y.append(activity)\n\nX = np.array(X)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Plotting\nplt.figure(figsize=(8, 6))\nfor activity in activities:\n    idx = np.array(y) == activity\n    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=activity, s=40)\n\nplt.title(\"PCA of Mean Acceleration per Sample\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PCA to reduce to 1 component\npca = PCA(n_components=1)\nX_pca = pca.fit_transform(X)\n\n# Plot\nplt.figure(figsize=(10, 5))\nfor activity in activities:\n    idx = np.array(y) == activity\n    plt.scatter([activity]*sum(idx), X_pca[idx], label=activity)\n\nplt.ylabel(\"PCA Component 1\")\nplt.title(\"1D PCA Projection of Acceleration Samples\")\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform PCA with 3 components\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X)\n\n# 3D scatter plot\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\nfor activity in activities:\n    idx = np.array(y) == activity\n    ax.scatter(X_pca[idx, 0], X_pca[idx, 1], X_pca[idx, 2], label=activity, s=40)\n\nax.set_title(\"3D PCA of Mean Acceleration per Sample\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_zlabel(\"PC3\")\nax.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. The plots reveals which activities are inherently separable (e.g., Walking vs. Laying) and which require finer           feature engineering (e.g., Sitting vs. Standing).\n2. Laying, Sitting, and Standing appear close together, suggesting they have similar acceleration patterns due to           limited movement.\n3. Walking, Walking Upstairs, and Walking Downstairs are more spread out, indicating greater variation in movement.\n4. Walking Upstairs and Downstairs may overlap somewhat with regular walking but show unique patterns due to the upward     or downward motion.","metadata":{}},{"cell_type":"code","source":"pip install tsfel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tsfel\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm\n\n# Load the raw acceleration signal files (each row is a 128-sample window)\nacc_x = pd.read_csv('/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt', delim_whitespace=True, header=None)\nacc_y = pd.read_csv('/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt', delim_whitespace=True, header=None)\nacc_z = pd.read_csv('/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt', delim_whitespace=True, header=None)\n\n# Load activity labels\ny_train = pd.read_csv('/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/y_train.txt', header=None)[0]\n\n# Configure TSFEL to extract temporal & statistical features (no spectral for speed)\ncfg = tsfel.get_features_by_domain(['temporal', 'statistical'])\n\nX_features = []\n\n# Loop through each sample (each row is a 128-sample window)\nfor i in tqdm(range(len(acc_x))):\n    try:\n        fx = tsfel.time_series_features_extractor(cfg, acc_x.iloc[i], sampling_frequency=50, verbose=0)\n        fy = tsfel.time_series_features_extractor(cfg, acc_y.iloc[i], sampling_frequency=50, verbose=0)\n        fz = tsfel.time_series_features_extractor(cfg, acc_z.iloc[i], sampling_frequency=50, verbose=0)\n        features = pd.concat([fx, fy, fz], axis=1)\n        X_features.append(features.values.flatten())\n    except Exception as e:\n        print(f\"Error at sample {i}: {e}\")\n\n# Convert to NumPy array and handle NaNs\nX_features = np.array(X_features)\nX_features = np.nan_to_num(X_features)\n\n# PCA to reduce to 2 components\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_features)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nfor label in np.unique(y_train):\n    idx = y_train == label\n    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=f'Activity {label}', alpha=0.5)\n\nplt.title(\"PCA on TSFEL Features (2D)\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\ndf_features = pd.DataFrame(X_features)\ncorr_matrix_tsfel = df_features.corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix_tsfel, cmap='coolwarm', center=0, xticklabels=False, yticklabels=False)\nplt.title(\"Correlation Matrix of TSFEL Features\")\nplt.show()\n\n# Identify redundant features\nhigh_corr_pairs_tsfel = np.where((np.abs(corr_matrix_tsfel) > 0.9) & (np.abs(corr_matrix_tsfel) < 1.0))\nredundant_provided = set()\n\nfor i, j in zip(*high_corr_pairs_tsfel):\n    redundant_provided.add((min(i, j), max(i, j)))\n\nprint(f\"TSFEL feature pairs (> 0.9 correlation): {len(redundant_provided)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Tsfel_Features=X_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Paths to provided features and labels\nX_path = \"/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/X_train.txt\"       # or original path if different\ny_path = \"/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/y_train.txt\"\nlabel_names_path = \"/kaggle/input/uci-har-dataset/UCI HAR Dataset/activity_labels.txt\"\n\n# Load features and labels\nX = pd.read_csv(X_path, delim_whitespace=True, header=None)\ny = pd.read_csv(y_path, header=None).values.flatten()\n\n# Map numeric labels to activity names\nlabel_map = pd.read_csv(label_names_path, delim_whitespace=True, header=None, index_col=0)\ny_named = [label_map.loc[label].values[0] for label in y]\n\n# Perform PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Plot\nplt.figure(figsize=(10, 6))\nfor activity in np.unique(y_named):\n    idx = np.array(y_named) == activity\n    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=activity, alpha=0.6)\n\nplt.title(\"PCA on Provided HAR Features\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load provided features\nX_provided = pd.read_csv(\"/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/X_train.txt\", delim_whitespace=True, header=None)\n\n# Correlation matrix\ncorr_matrix_provided = X_provided.corr()\n\n# Visualize\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix_provided, cmap='coolwarm', cbar=True)\nplt.title(\"Correlation Matrix of Provided Features\")\nplt.show()\n\n# Identify redundant features\nhigh_corr_pairs_provided = np.where((np.abs(corr_matrix_provided) > 0.9) & (np.abs(corr_matrix_provided) < 1.0))\nredundant_provided = set()\n\nfor i, j in zip(*high_corr_pairs_provided):\n    redundant_provided.add((min(i, j), max(i, j)))\n\nprint(f\"Provided Redundant feature pairs (> 0.9 correlation): {len(redundant_provided)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load provided features\nX_provided = pd.read_csv(\"/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/X_train.txt\", delim_whitespace=True, header=None)\n\n# Correlation matrix\ncorr_matrix = X_provided.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n# Find features with correlation greater than 0.9\nto_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n\nprint(f\" Number of highly correlated features to drop (>0.9): {len(to_drop)}\")\nprint(f\" Dropping columns: {to_drop[:10]}{'...' if len(to_drop) > 10 else ''}\")\n\n# Drop the features\nX_reduced = X_provided.drop(columns=to_drop).reset_index(drop=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nbase_path = '/kaggle/input/d/dinesh168/collected-data/Collected data'\nactivities = ['Laying', 'Sitting', 'Standing', 'Walking', 'Walking_Downstairs', 'Walking_Upstairs']\n\nplt.figure(figsize=(18, 8))\ny_min, y_max = float('inf'), float('-inf')\n\nfor i, activity in enumerate(activities):\n    path = os.path.join(base_path, activity)\n    file = os.listdir(path)[0]\n    df = pd.read_csv(os.path.join(path, file), header=None)\n\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n    df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n    df = df.iloc[:]\n    \n    y_min = min(y_min, df.min().min())\n    y_max = max(y_max, df.max().max())\n\n    plt.subplot(2, 3, i + 1)\n    plt.plot(df['acc_x'], label='X', alpha=0.7)\n    plt.plot(df['acc_y'], label='Y', alpha=0.7)\n    plt.plot(df['acc_z'], label='Z', alpha=0.7)\n    plt.title(activity)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Acceleration\")\n    plt.ylim(y_min, y_max) \n    plt.grid(True)\n    if i == 0:\n        plt.legend(loc='upper right')\n\nplt.suptitle(\"Accelerometer Waveforms for Each Activity\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(18, 8))\n\nfor i, activity in enumerate(activities):\n    path = os.path.join(base_path, activity)\n    file = os.listdir(path)[0]\n    df = pd.read_csv(os.path.join(path, file), header=None)\n\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n    df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n    total_acc = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n\n    plt.subplot(2, 3, i + 1)\n    plt.plot(total_acc, color='darkviolet')\n    plt.title(f\"{activity}\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"||acc||\")\n    plt.grid(True)\n\nplt.suptitle(\"Total Acceleration Magnitude for Each Activity\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(18, 8))\n\nfor i, activity in enumerate(activities):\n    path = os.path.join(base_path, activity)\n    file = os.listdir(path)[0]\n    df = pd.read_csv(os.path.join(path, file), header=None)\n\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df.dropna(inplace=True)\n    df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n\n    ax = fig.add_subplot(2, 3, i + 1, projection='3d')\n    ax.plot(df['acc_x'], df['acc_y'], df['acc_z'])\n    ax.set_title(activity)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\nplt.suptitle(\"3D Trajectories of Accelerometer Data\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}