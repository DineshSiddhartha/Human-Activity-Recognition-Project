{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# --------------------------\n# Load UCI HAR dataset\nX = pd.read_csv('/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None)\ny = pd.read_csv('/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/y_train.txt', header=None)[0] - 1  # Make labels 0-indexed\n\n# Standardize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Convert to torch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n\n# --------------------------\n# Define neural network\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# --------------------------\n# Hyperparameters\ninput_size = X.shape[1]\nhidden_size = 64\nnum_classes = len(np.unique(y))\nnum_epochs = 30\nbatch_size = 64\nlearning_rate = 0.001\n\n# --------------------------\n# Create model, loss, optimizer\nmodel = SimpleNN(input_size, hidden_size, num_classes)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# --------------------------\n# Train\nmodel.train()\nfor epoch in range(num_epochs):\n    permutation = torch.randperm(X_train_tensor.size(0))\n    for i in range(0, X_train_tensor.size(0), batch_size):\n        indices = permutation[i:i+batch_size]\n        batch_x, batch_y = X_train_tensor[indices].to(device), y_train_tensor[indices].to(device)\n\n        # Forward pass\n        outputs = model(batch_x)\n        loss = criterion(outputs, batch_y)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 5 == 0 or epoch == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# --------------------------\n# Evaluate\nmodel.eval()\nwith torch.no_grad():\n    y_pred_logits = model(X_test_tensor.to(device))\n    y_pred = torch.argmax(y_pred_logits, dim=1).cpu().numpy()\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\nrecall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\nf1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n\nprint(\"\\n Test Set Performance:\")\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tsfel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\n# Ignore all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nimport tsfel\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\n\n# ---------- Load signals ----------\ndef load_signals(folder, signal_name):\n    path = f\"/kaggle/input/uci-har-dataset/UCI HAR Dataset/{folder}/Inertial Signals/{signal_name}\"\n    return np.loadtxt(path)\n\nax = np.vstack([load_signals('train', 'total_acc_x_train.txt'), load_signals('test', 'total_acc_x_test.txt')])\nay = np.vstack([load_signals('train', 'total_acc_y_train.txt'), load_signals('test', 'total_acc_y_test.txt')])\naz = np.vstack([load_signals('train', 'total_acc_z_train.txt'), load_signals('test', 'total_acc_z_test.txt')])\n\ny = np.concatenate([\n    np.loadtxt(\"/kaggle/input/uci-har-dataset/UCI HAR Dataset/train/y_train.txt\").astype(int) - 1,\n    np.loadtxt(\"/kaggle/input/uci-har-dataset/UCI HAR Dataset/test/y_test.txt\").astype(int) - 1,\n])\n\n# ---------- Configure TSFEL ----------\ncfg = tsfel.get_features_by_domain()\n\ndef extract_features_tsfel(signal_array, fs=50):\n    features_list = []\n    print(f\"Starting TSFEL feature extraction for {len(signal_array)} signals...\")\n    for idx, row in enumerate(signal_array):\n        df_row = pd.DataFrame(row)\n        features = tsfel.time_series_features_extractor(cfg, df_row, fs=fs, verbose=0)\n        feature_values = np.nan_to_num(features.values[0])\n        features_list.append(feature_values)\n        if (idx + 1) % 500 == 0 or (idx + 1) == len(signal_array):\n            print(f\"Extracted features for {idx + 1}/{len(signal_array)} samples\")\n    return np.array(features_list)\n\n# ---------- Extract features ----------\nprint(\"Extracting TSFEL features for ax...\")\nfeat_ax = extract_features_tsfel(ax)\nprint(\" ax done!\\n\")\n\nprint(\"Extracting TSFEL features for ay...\")\nfeat_ay = extract_features_tsfel(ay)\nprint(\" ay done!\\n\")\n\nprint(\"Extracting TSFEL features for az...\")\nfeat_az = extract_features_tsfel(az)\nprint(\" az done!\\n\")\n\nX = np.hstack([feat_ax, feat_ay, feat_az])\nprint(\" All feature extraction complete!\")\n\n# ---------- Scale ----------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ---------- Train/test split ----------\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n\n# ---------- PyTorch NN ----------\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out\n\n# ---------- Device setup ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\" Using device: {device}\")\n\n# Convert to torch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n\n# Model, loss, optimizer\ninput_dim = X_train.shape[1]\nnum_classes = len(np.unique(y))\nmodel = SimpleNN(input_dim, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nprint(\" Training started...\")\nfor epoch in range(15):\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 3 == 0:\n        _, predicted = torch.max(outputs, 1)\n        acc = (predicted == y_train_tensor).float().mean()\n        print(f\"Epoch [{epoch+1}/15], Loss: {loss.item():.4f}, Train Acc: {acc.item():.4f}\")\n\nprint(\" Training completed.\")\n\n# Evaluate\nmodel.eval()\nwith torch.no_grad():\n    outputs_test = model(X_test_tensor)\n    _, y_pred_test = torch.max(outputs_test, 1)\n\ny_pred_test_np = y_pred_test.cpu().numpy()\ny_test_np = y_test_tensor.cpu().numpy()\n\nprint(\"\\n Test Performance:\")\nprint(f\"Accuracy: {accuracy_score(y_test_np, y_pred_test_np):.4f}\")\nprint(f\"Precision: {precision_score(y_test_np, y_pred_test_np, average='weighted'):.4f}\")\nprint(f\"Recall: {recall_score(y_test_np, y_pred_test_np, average='weighted'):.4f}\")\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test_np, y_pred_test_np))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = '/kaggle/input/d/dinesh168/collected-data/Collected data'\nactivities = ['Laying', 'Sitting', 'Standing', 'Walking', 'Walking_Downstairs', 'Walking_Upstairs']\n\nfeature_list = []\ntrue_labels = []\nfile_names = []\n\nfor activity in activities:\n    path = os.path.join(base_path, activity)\n    for file in os.listdir(path):\n        df = pd.read_csv(os.path.join(path, file), header=None)\n        df = df.apply(pd.to_numeric, errors='coerce')\n        df.dropna(inplace=True)\n        df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n\n        if len(df) < 128:\n            continue\n        window = df.iloc[:128]\n\n        tsfel_features_all_axes = []\n        for axis in ['acc_x', 'acc_y', 'acc_z']:\n            sig_df = pd.DataFrame(window[axis].values)\n            tsfel_feat_df = tsfel.time_series_features_extractor(cfg, sig_df, verbose=0)\n            tsfel_feat = np.nan_to_num(tsfel_feat_df.values.flatten())\n            tsfel_features_all_axes.append(tsfel_feat)\n\n        final_features = np.concatenate(tsfel_features_all_axes)\n        feature_list.append(final_features)\n        true_labels.append(activity)\n        file_names.append(file)\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to device\nmodel = model.to(device)\n# Convert\nX_new = np.array(feature_list)\nX_new_scaled = scaler.transform(X_new)\nX_new_tensor = torch.tensor(X_new_scaled, dtype=torch.float32).to(device)\n\n# Predict\nwith torch.no_grad():\n    outputs_new = model(X_new_tensor)\n    _, y_pred_new = torch.max(outputs_new, 1)\n\n# Move predictions back to CPU before using numpy()\ny_pred_new_cpu = y_pred_new.cpu().numpy()\n\n# Show results\nlabel_map = {\n    0: 'Walking',\n    1: 'Walking_Upstairs',\n    2: 'Walking_Downstairs',\n    3: 'Sitting',\n    4: 'Standing',\n    5: 'Laying'\n}\n\nprint(\"\\n Predictions on collected data:\")\nfor fname, pred_idx, true_label in zip(file_names, y_pred_new_cpu, true_labels):\n    pred_label = label_map.get(pred_idx, \"Unknown\")\n    print(f\"{fname}: Predicted = {pred_label}, Actual = {true_label}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n\n# Convert predictions and true labels to numpy arrays\ny_pred_new_np = y_pred_new.cpu().numpy()\n# Map true activity labels to numeric labels using label_map\ntrue_labels_numeric = []\n\n# Invert label_map for easy lookup\ninv_label_map = {v: k for k, v in label_map.items()}\n\nfor lbl in true_labels:\n    true_labels_numeric.append(inv_label_map.get(lbl, -1))\n\ntrue_labels_numeric = np.array(true_labels_numeric)\n\n# Compute metrics\nacc = accuracy_score(true_labels_numeric, y_pred_new_np)\nprec = precision_score(true_labels_numeric, y_pred_new_np, average='weighted', zero_division=0)\nrec = recall_score(true_labels_numeric, y_pred_new_np, average='weighted', zero_division=0)\nconf_mat = confusion_matrix(true_labels_numeric, y_pred_new_np)\n\n# Print predictions file-wise\nprint(\"\\n Predictions on collected data:\")\nfor fname, pred_idx, true_lbl in zip(file_names, y_pred_new_np, true_labels):\n    pred_label = label_map.get(pred_idx, \"Unknown\")\n    print(f\"{fname}: Predicted = {pred_label}, Actual = {true_lbl}\")\n\n# Print metrics\nprint(\"\\n Evaluation Metrics on collected data:\")\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall: {rec:.4f}\")\nprint(\"Confusion Matrix:\\n\", conf_mat)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tsfel\nfrom sklearn.preprocessing import StandardScaler\n\nbase_path = '/kaggle/input/d/dinesh168/collected-data/Collected data'\nactivities = ['Laying', 'Sitting', 'Standing', 'Walking', 'Walking_Downstairs', 'Walking_Upstairs']\n\nfeature_list = []\nlabel_list = []\n\ncfg = tsfel.get_features_by_domain()\n\nlabel_map = {\n    0: 'Walking',\n    1: 'Walking_Upstairs',\n    2: 'Walking_Downstairs',\n    3: 'Sitting',\n    4: 'Standing',\n    5: 'Laying'\n}\ninv_label_map = {v: k for k, v in label_map.items()}\n\nfor activity in activities:\n    path = os.path.join(base_path, activity)\n    for file in os.listdir(path):\n        df = pd.read_csv(os.path.join(path, file), header=None)\n        df = df.apply(pd.to_numeric, errors='coerce')\n        df.dropna(inplace=True)\n        df.columns = ['acc_x', 'acc_y', 'acc_z'][:df.shape[1]]\n\n        if len(df) < 128:\n            continue\n        window = df.iloc[:128]\n\n        tsfel_features_all_axes = []\n        for axis in ['acc_x', 'acc_y', 'acc_z']:\n            sig_df = pd.DataFrame(window[axis].values)\n            tsfel_feat_df = tsfel.time_series_features_extractor(cfg, sig_df, verbose=0)\n            tsfel_feat = np.nan_to_num(tsfel_feat_df.values.flatten())\n            tsfel_features_all_axes.append(tsfel_feat)\n\n        final_features = np.concatenate(tsfel_features_all_axes)\n        feature_list.append(final_features)\n        label_list.append(inv_label_map.get(activity, -1))\n\nX_collected = np.array(feature_list)\ny_collected = np.array(label_list)\n\n# Combine features and labels\nX_combined = np.vstack([X_har, X_collected])\ny_combined = np.concatenate([y_har, y_collected])\n\n# Re-scale all combined features\nscaler_combined = StandardScaler()\nX_combined_scaled = scaler_combined.fit_transform(X_combined)\n\nfrom sklearn.model_selection import train_test_split\n\n# Shuffle and split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_combined_scaled, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Tensors\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n\ninput_dim = X_train.shape[1]\nnum_classes = len(np.unique(y_combined))\nmodel = SimpleNN(input_dim, num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(\" Training started...\")\nfor epoch in range(15):\n    model.train()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 3 == 0:\n        _, predicted = torch.max(outputs, 1)\n        acc = (predicted == y_train_tensor).float().mean()\n        print(f\"Epoch [{epoch+1}/15], Loss: {loss.item():.4f}, Train Acc: {acc.item():.4f}\")\n\nprint(\" Training completed.\")\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n\nmodel.eval()\nwith torch.no_grad():\n    outputs_test = model(X_test_tensor)\n    _, y_pred_test = torch.max(outputs_test, 1)\n\ny_pred_test_np = y_pred_test.cpu().numpy()\ny_test_np = y_test_tensor.cpu().numpy()\n\nprint(\"\\n Combined Test Performance:\")\nprint(f\"Accuracy: {accuracy_score(y_test_np, y_pred_test_np):.4f}\")\nprint(f\"Precision: {precision_score(y_test_np, y_pred_test_np, average='weighted'):.4f}\")\nprint(f\"Recall: {recall_score(y_test_np, y_pred_test_np, average='weighted'):.4f}\")\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test_np, y_pred_test_np))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}